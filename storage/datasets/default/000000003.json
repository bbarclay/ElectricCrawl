{
	"title": "A/B Testing - Builder.io",
	"url": "https://www.builder.io/c/docs/abtesting",
	"html": "A/B Testing\n\nPro plans\n\nA/B testing is a data-driven approach to testing and measuring different content and experiences to help you make impactful improvements to your site or app. \n\nPrerequisites\n\nTo use A/B testing on a content entry, that content must be created and managed in Builder. Content created elsewhere cannot use Builder's A/B Testing feature.\n\nBuilder's A/B testing advantages\n\nThe advantages of doing A/B testing with Builder include:\n\nSeamless conversion tracking: Monitor conversions right within the intuitive UI alongside your Builder content.\nLimitless comparisons: Execute numerous comparisons, tailoring your tests to your specific needs.\nSEO friendly: Use search engine optimization efforts while conducting A/B tests.\nUninterrupted user experience: No DOM/UI blocking and unwanted content flashes.\nStraightforward operation: No coding required—you don't have to be a developer to engage in effective A/B testing.\nEfficient Compression: Leverage gzip compression for optimized performance, minimizing added weight even with multiple test groups.\n\nDespite multiple content pieces being initially sent with the HTML, gzip deflation efficiently eliminates redundancy, resulting in a mere 5-10% size increase. This ensures your page remains fast. Builder extends A/B testing support to all frameworks, including static ones such as Gatsby and Nuxt.\n\nCreating variations\n\nBegin A/B testing by crafting distinct variations of a page or piece of content so Builder can collect metrics for each version.\n\nTip: Depending on whether a content entry has already been published, the Builder UI workflow might prompt you to make a duplicate content entry.\n\nIf the content entry that you'd like to test isn't published yet, you don't need to create a duplicate entry and you might notice just one entry in the content list when you're done.\n\nBoth workflows are correct and serve to maintain insight integrity.\n\nTo create an A/B test variation:\n\nIn the content entry, click the A/B Tests tab.\nClick the Add A/B Test Variation button.\nRename your new variation—initially labeled Variation 0 by default—and fine-tune the test ratio. For more variants to perform multi-variate testing, repeat this process.\nGo back to the Edit tab to toggle between variations and make adjustments.\nWhen you're ready, click the Publish button to initiate the test. If you'd rather schedule it for a future launch, visit Scheduling Content.\n\nThe video below goes through this process in a previously unpublished entry. If your content entry is already published, the Builder UI will guide you.\n\nImportant points to remember\n\nWhen A/B Testing, keep these things in mind:\n\nThe default variation, known as the control, serves as the baseline exposed to search engines, users with JavaScript disabled, and those with tracking deactivated.\nOnce a test is live and receiving traffic, refrain from introducing new variants or removing existing ones. This maintains the integrity of your test results and safeguards them from the influence of previous tests or outdated content versions.\nAllocation follows a random pattern based on the test ratio, tracked with cookies. However, users with tracking disabled remain unassigned to a test group.\n\nFor A/B testing with Symbols, visit Symbols.\n\nInterpreting A/B test metrics\n\nLeverage the data from your A/B test to determine the winning variation. Scrutinize conversion statistics across variations, focusing on metrics like conversion rate, total conversions, or conversion value, tailored to your priorities. Allow the test sufficient time to accumulate data and visitors for accurate conclusions.\n\nTip: For conversion metrics on custom tech stacks you'll need to integrate conversion tracking. Shopify-hosted stores benefit from automatic integration.\n\nTo evaluate the performance of your A/B test:\n\nIn the Visual Editor, go to the A/B Tests tab.\nClick View Results beneath the sliders. This directs you to the Insights tab, where you can access comprehensive test results. Initial stages may show limited visitor data, but with time, you'll gain substantial insights into your test's progress.\n\nThe following is a screenshot of the Insights tab that shows data such as Impressions, Clicks, and Clickthrough rate:\n\nBuilder calculates conversions based on impressions; in this way, an impression of Builder content is all that's necessary to lead to a trackable conversion.\n\nConcluding an A/B Test\n\nOnce you've determined the winning variation, it's time to implement the improvements. You have a couple of options that you manage from the A/B Tests tab.\n\nKeeping all variations but delivering the winner\n\nTip: Use this option if you think you might want the test variations later.\n\nThis option retains all variations but delivers only the winning variation to new visitors.\n\nAdjust the ratios within the A/B Tests tab to exclusively display the winning variation to new visitors. This ensures that your audience immediately experiences the best UI, and you can keep the other variations for future use.\n\nNote that users who visited the site while the test was active and were assigned a losing variation's cookie will continue to get the losing variation until their cookie expires or is cleared.\n\nEnding an A/B Test while removing variations\n\nThis option conclusively ends the test.\n\nIf you don't duplicate this entry before picking a winner, the other variations are not copied. However, if you duplicate this entry first and then come back to choose the winner, the variations are ported to the duplicate.\n\nThe following table outlines what happens to variations depending on whether you choose the winner before or after duplicating:\n\nWhen you choose winner\tWhat happens to variations\n\nChoose Winner before duplicating\n\n\t\n\nTest ends. Test variations are not ported to duplicate entry. You'll have to create new variations if you need more A/B tests.\n\n\n\n\nChoose Winner after duplicating\n\n\t\n\nDuplicate is created before ending test. This means variations are copied to duplicate entry.\n\nTo end an A/B test completely:\n\nSelect the Choose Winning Variation option within the A/B Tests tab.\nIn the dialogue that opens, read the details so your choice is informed.\nClick the End Test button.\n\nAfter ending the test, Builder designates the winner with a checkmark icon. The following video demonstrates choosing the winner.\n\nStatistical significance\n\nAs your test accumulates data, Builder computes statistical significance. A checkmark next to a variant's name in the table indicates its impact on conversion, with a gray checkmark representing a p-value within a 90% confidence interval and a green checkmark exceeding 95%.\n\nNote that to use this feature, you must integrate conversion tracking.\n\nExamples of A/B tests\n\nThe following are some examples of features you can A/B test:\n\nGeneral Area\tSpecific area\tConcept\tExample\n\nContent\n\n\t\n\nHeaders\n\n\t\n\nVary headers to find out what visitors respond to most\n\n\t\n\n\"New Swimwear is Here!\" or \"Bathing Suits Galore\"\n\n\n\n\nContent\n\n\t\n\nCopy and designs\n\n\t\n\nTry different copy styles according to your ideal customer persona\n\n\t\n\n\"We're digging summer!\" or \"Luxuriate in Paradise\"\n\n\n\n\nDesign\n\n\t\n\nColors, design features, fonts\n\n\t\n\nExperiment with button size, shape, style to find out what encourages clicks\n\n\t\n\nRed button with all caps copy or a branded color using a minimal lowercase font\n\n\n\n\nDesign\n\n\t\n\nCTA position\n\n\t\n\nDifferent button locations in a content entry\n\n\t\n\nBottom right or bottom left placement.\n\n\n\n\nDesign\n\n\t\n\nHero image\n\n\t\n\nDifferent hero images to find out which gets more clicks\n\n\t\n\nA hero with a person leaping into a blue sea or a hero of just the sea, without the person\n\n\n\n\nFunctionality\n\n\t\n\nForm\n\n\t\n\nExperiment different form formats\n\n\t\n\nA form with multiple choice questions or free-form text boxes for written answers\n\n\n\n\nFunctionality\n\n\t\n\nResponsiveness\n\n\t\n\nHiding or showing a feature based on device-type to find out if users can use the feature as effectively\n\n\t\n\nPlacing search in the hamburger menu or placing it at the top of the page on mobile devices\n\nWhat's next\n\nIn addition to running A/B tests in Builder, you can also display content based on a schedule. For more information, visit Scheduling Content."
}